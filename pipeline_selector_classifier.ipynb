{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier, \n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel, \n",
    "    RFE, \n",
    "    SelectKBest, \n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    VarianceThreshold\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def run_ml_pipeline(df, target_cols, n_features=30):\n",
    "    \"\"\"\n",
    "    Run complete ML pipeline with multiple classifiers and feature selectors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    target_cols : list\n",
    "        List of target columns to exclude from features\n",
    "    n_features : int\n",
    "        Number of features to select (default: 30)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare results dictionary\n",
    "    results = {\n",
    "        'classifier': [],\n",
    "        'selector': [],\n",
    "        'sampler': [],\n",
    "        'n_features': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'roc_auc': [],\n",
    "        'selected_features': [],\n",
    "        'training_time': [],\n",
    "        'confusion_matrix': []\n",
    "    }\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    df_encoded = df.copy()\n",
    "    for column in categorical_columns:\n",
    "        df_encoded[column] = le.fit_transform(df_encoded[column].astype(str))\n",
    "\n",
    "    X = df_encoded.drop(columns=target_cols, errors='ignore')\n",
    "    y = df_encoded['target'].astype(int)  #\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define samplers for imbalanced data\n",
    "    samplers = {\n",
    "        'SMOTE': SMOTE(random_state=42),\n",
    "        'ADASYN': ADASYN(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Define classifiers with optimized parameters\n",
    "    classifiers = {\n",
    "        'GradientBoosting': GradientBoostingClassifier(n_estimators=170, learning_rate=0.1, \n",
    "                                                      random_state=42, min_samples_split=8, \n",
    "                                                      min_samples_leaf=2),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(n_estimators=100, random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'SVC': SVC(probability=True, random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'ExtraTrees': ExtraTreesClassifier(random_state=42),\n",
    "        'KNeighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "        'RidgeClassifier': RidgeClassifier(random_state=42),\n",
    "        'Bagging': BaggingClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Define feature selectors\n",
    "    base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "    selectors = {\n",
    "        'SelectFromModel(GB)': SelectFromModel(\n",
    "            GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "            max_features=n_features\n",
    "        ),\n",
    "        'SelectFromModel(RF)': SelectFromModel(\n",
    "            RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            max_features=n_features\n",
    "        ),\n",
    "        'SelectFromModel(XGB)': SelectFromModel(\n",
    "            XGBClassifier(n_estimators=100, random_state=42),\n",
    "            max_features=n_features\n",
    "        ),\n",
    "        'RFE': RFE(\n",
    "            estimator=base_estimator,\n",
    "            n_features_to_select=n_features\n",
    "        ),\n",
    "        'SelectKBest(f_classif)': SelectKBest(\n",
    "            score_func=f_classif,\n",
    "            k=n_features\n",
    "        ),\n",
    "        'SelectKBest(mutual_info)': SelectKBest(\n",
    "            score_func=mutual_info_classif,\n",
    "            k=n_features\n",
    "        ),\n",
    "        'VarianceThreshold': VarianceThreshold(threshold=0.01)\n",
    "    }\n",
    "\n",
    "    # Run all combinations\n",
    "    for sampler_name, sampler in samplers.items():\n",
    "        print(f\"\\nUsing {sampler_name} for imbalanced data handling\")\n",
    "        \n",
    "        # Apply sampling\n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Verificar as classes presentes após o resampling\n",
    "        print(f\"Classes after resampling: {np.unique(y_train_resampled)}\")\n",
    "        \n",
    "        for clf_name, clf in classifiers.items():\n",
    "            for selector_name, selector in selectors.items():\n",
    "                print(f\"\\nRunning {clf_name} with {selector_name} and {sampler_name}\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # Feature selection\n",
    "                    X_train_selected = selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "                    X_test_selected = selector.transform(X_test)\n",
    "                    \n",
    "                    # Check if we have enough features\n",
    "                    if X_train_selected.shape[1] < 2:\n",
    "                        print(f\"Warning: {selector_name} selected too few features. Skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Training with sample weights\n",
    "                    weights = np.where(y_train_resampled == 1, 1.6, 1)\n",
    "                    if isinstance(clf, (SVC, KNeighborsClassifier, GaussianNB)):\n",
    "                        clf.fit(X_train_selected, y_train_resampled)\n",
    "                    else:\n",
    "                        clf.fit(X_train_selected, y_train_resampled, sample_weight=weights)\n",
    "                    \n",
    "                    # Predictions\n",
    "                    predictions = clf.predict(X_test_selected)\n",
    "                    if hasattr(clf, \"predict_proba\"):\n",
    "                        probabilities = clf.predict_proba(X_test_selected)[:, 1]\n",
    "                    else:\n",
    "                        probabilities = clf.predict(X_test_selected)\n",
    "                    \n",
    "                    # Get metrics\n",
    "                    report = classification_report(y_test, predictions, output_dict=True)\n",
    "                    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "                    roc_auc = roc_auc_score(y_test, probabilities)\n",
    "                    \n",
    "                    # Get selected features\n",
    "                    if hasattr(selector, 'get_support'):\n",
    "                        selected_features = X.columns[selector.get_support()].tolist()\n",
    "                    else:\n",
    "                        selected_features = X.columns[:X_train_selected.shape[1]].tolist()\n",
    "                    \n",
    "                    # Store results\n",
    "                    results['classifier'].append(clf_name)\n",
    "                    results['selector'].append(selector_name)\n",
    "                    results['sampler'].append(sampler_name)\n",
    "                    results['n_features'].append(len(selected_features))\n",
    "                    results['accuracy'].append(report['accuracy'])\n",
    "                    \n",
    "                    # Verificar se a classe '1' existe no relatório\n",
    "                    if '1' in report:\n",
    "                        results['precision'].append(report['1']['precision'])\n",
    "                        results['recall'].append(report['1']['recall'])\n",
    "                        results['f1'].append(report['1']['f1-score'])\n",
    "                    else:\n",
    "                        # Usar a classe positiva (assumindo classificação binária)\n",
    "                        positive_class = str(max(map(int, report.keys() - {'accuracy', 'macro avg', 'weighted avg'})))\n",
    "                        results['precision'].append(report[positive_class]['precision'])\n",
    "                        results['recall'].append(report[positive_class]['recall'])\n",
    "                        results['f1'].append(report[positive_class]['f1-score'])\n",
    "                    \n",
    "                    results['roc_auc'].append(roc_auc)\n",
    "                    results['selected_features'].append(selected_features)\n",
    "                    results['training_time'].append(time.time() - start_time)\n",
    "                    results['confusion_matrix'].append(conf_matrix)\n",
    "                    \n",
    "                    # Print detailed report\n",
    "                    print(f\"\\nResults for {clf_name} with {selector_name} and {sampler_name}\")\n",
    "                    print(\"=\"*80)\n",
    "                    print(f\"Number of selected features: {len(selected_features)}\")\n",
    "                    print(f\"\\nClassification Report:\")\n",
    "                    print(classification_report(y_test, predictions))\n",
    "                    print(f\"\\nConfusion Matrix:\")\n",
    "                    print(conf_matrix)\n",
    "                    print(f\"\\nROC AUC: {roc_auc:.4f}\")\n",
    "                    print(f\"\\nTraining Time: {results['training_time'][-1]:.2f} seconds\")\n",
    "                    print(\"\\nTop 10 Selected Features:\")\n",
    "                    print(selected_features[:10])\n",
    "                    print(\"=\"*80)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {clf_name} and {selector_name}:\")\n",
    "                    print(f\"Detailed error: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df.to_csv(f'ml_pipeline_results_{timestamp}.csv', index=False)\n",
    "    \n",
    "    # Print summary of best results\n",
    "    print(\"\\nTop 10 Models by ROC AUC:\")\n",
    "    print(results_df.sort_values('roc_auc', ascending=False).head(10)[\n",
    "        ['classifier', 'selector', 'sampler', 'n_features', 'accuracy', \n",
    "         'precision', 'recall', 'f1', 'roc_auc', 'training_time'\n",
    "    ]])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Execute o pipeline\n",
    "target_columns = [\"target\", \"T_score_fn\", \"T_score_tf\", \"T_score_sp\",\n",
    "                 \"DXXSPN_DXXOSBMD\", \"DXXFEM_DXXNKBMD\", \"DXXFEM_DXXOFBMD\"]\n",
    "\n",
    "# Primeiro, vamos verificar os dados\n",
    "print(\"Verificando os dados antes de executar o pipeline:\")\n",
    "print(\"\\nDistribuição da variável target:\")\n",
    "print(df_final['target'].value_counts())\n",
    "print(\"\\nTipos de dados das colunas:\")\n",
    "print(df_final.dtypes)\n",
    "\n",
    "# Executar o pipeline\n",
    "results_df = run_ml_pipeline(df_final, target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar resultados por diferentes métricas\n",
    "print(\"\\nTop 5 modelos por ROC AUC:\")\n",
    "print(results_df.sort_values('roc_auc', ascending=False).head()[\n",
    "    ['classifier', 'selector', 'sampler', 'n_features', 'roc_auc', 'f1', 'training_time']\n",
    "])\n",
    "\n",
    "print(\"\\nTop 5 modelos por F1-Score:\")\n",
    "print(results_df.sort_values('f1', ascending=False).head()[\n",
    "    ['classifier', 'selector', 'sampler', 'n_features', 'roc_auc', 'f1', 'training_time']\n",
    "])\n",
    "\n",
    "# Criar uma métrica composta\n",
    "results_df['score_composto'] = (\n",
    "    0.7 * results_df['roc_auc'] + \n",
    "    0.3 * results_df['f1'] \n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 modelos por Score Composto:\")\n",
    "print(results_df.sort_values('score_composto', ascending=False).head()[\n",
    "    ['classifier', 'selector', 'sampler', 'n_features', 'roc_auc', 'f1', 'training_time', 'score_composto']\n",
    "])\n",
    "\n",
    "# Análise detalhada do melhor modelo\n",
    "melhor_modelo = results_df.sort_values('score_composto', ascending=False).iloc[0]\n",
    "print(\"\\nAnálise detalhada do melhor modelo:\")\n",
    "print(f\"Classifier: {melhor_modelo['classifier']}\")\n",
    "print(f\"Selector: {melhor_modelo['selector']}\")\n",
    "print(f\"Sampler: {melhor_modelo['sampler']}\")\n",
    "print(f\"Número de features: {melhor_modelo['n_features']}\")\n",
    "print(f\"ROC AUC: {melhor_modelo['roc_auc']:.4f}\")\n",
    "print(f\"F1-Score: {melhor_modelo['f1']:.4f}\")\n",
    "print(f\"Tempo de treinamento: {melhor_modelo['training_time']:.2f} segundos\")\n",
    "print(\"\\nFeatures selecionadas:\")\n",
    "print(melhor_modelo['selected_features'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
